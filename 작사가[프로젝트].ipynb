{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNoKdgTiwGB3bQI/9ecR9Gd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ohsoo46/AIFFEL_QUEST/blob/main/%EC%9E%91%EC%82%AC%EA%B0%80%5B%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 드라이브 마운트\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5JKSZI0War6p",
        "outputId": "f4631159-74ef-4735-a563-cb610887b131"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "6JS-M2j8aj5-"
      },
      "outputs": [],
      "source": [
        "import os, glob, re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lyrics_dir = \"/content/drive/MyDrive/ds6_4SEC/lyrics\"\n",
        "txt_list = glob.glob(os.path.join(lyrics_dir, \"*.txt\"))\n",
        "\n",
        "raw_corpus = []\n",
        "for p in txt_list:\n",
        "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
        "        raw_corpus.extend(f.read().splitlines())\n",
        "\n",
        "print(\"txt files:\", len(txt_list))\n",
        "print(\"raw lines:\", len(raw_corpus))\n",
        "print(\"sample:\", raw_corpus[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhhDUdjYar9K",
        "outputId": "290040b6-a8e6-4f63-9f7d-70083a79e313"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "txt files: 49\n",
            "raw lines: 187088\n",
            "sample: ['[Verse 1]', 'They come from everywhere', 'A longing to be free']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence: str) -> str:\n",
        "    sentence = sentence.lower().strip()\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r\"[^a-z?.!,]+\", \" \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    sentence = \"<start> \" + sentence + \" <end>\"\n",
        "    return sentence\n",
        "\n",
        "corpus = [preprocess_sentence(s) for s in raw_corpus if isinstance(s, str) and s.strip()]\n",
        "print(\"corpus:\", len(corpus))\n",
        "print(\"sample:\", corpus[:2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz_qDtqkaw67",
        "outputId": "1f0644c9-e5ec-48ac-c888-bf4018a3675e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "corpus: 175960\n",
            "sample: ['<start> verse <end>', '<start> they come from everywhere <end>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(corpus, vocab_size=12000, max_len=15):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=vocab_size,\n",
        "        filters=\"\",     # 이미 preprocess에서 정제했으니 filters 비우기\n",
        "        oov_token=\"<unk>\"\n",
        "    )\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tensor, padding=\"post\", maxlen=max_len\n",
        "    )\n",
        "    return tensor, tokenizer\n",
        "\n",
        "MAX_LEN = 15\n",
        "VOCAB_SIZE = 12000\n",
        "\n",
        "tensor, tokenizer = tokenize(corpus, vocab_size=VOCAB_SIZE, max_len=MAX_LEN)\n",
        "print(\"tensor shape:\", tensor.shape)\n",
        "print(\"vocab size (actual):\", min(VOCAB_SIZE, len(tokenizer.word_index)+1))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCreUyt_azUD",
        "outputId": "41fc56e3-d51c-4844-b925-5f550ecbfca0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor shape: (175960, 15)\n",
            "vocab size (actual): 12000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "enc_inputs  = tensor[:, :-1]  # (N, 14)\n",
        "dec_targets = tensor[:,  1:]  # (N, 14)\n",
        "\n",
        "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
        "    enc_inputs, dec_targets, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(enc_train.shape, dec_train.shape, enc_val.shape, dec_val.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbHUj5yGa1qo",
        "outputId": "d7104b88-f064-4155-d16e-81a9f1c249d3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(140768, 14) (140768, 14) (35192, 14) (35192, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class TextGenerator(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
        "        self.linear = tf.keras.layers.Dense(vocab_size, dtype=\"float32\")\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.rnn_1(x)\n",
        "        x = self.rnn_2(x)\n",
        "        x = self.linear(x)   # logits\n",
        "        return x\n",
        "\n",
        "\n",
        "# 하이퍼파라미터\n",
        "embedding_size = 256\n",
        "hidden_size = 512\n",
        "vocab_size = min(VOCAB_SIZE, len(tokenizer.word_index) + 1)\n",
        "\n",
        "# 모델 생성\n",
        "model = TextGenerator(vocab_size, embedding_size, hidden_size)\n",
        "\n",
        "# loss (PAD=0 마스킹)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
        "\n",
        "def masked_loss(y_true, y_pred):\n",
        "    loss = loss_fn(y_true, y_pred)\n",
        "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
        "    loss = loss * mask\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=masked_loss\n",
        ")\n",
        "\n",
        "print(\" model 정의 완료\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwOz-Ug2i16_",
        "outputId": "cf1e9d8c-2f4b-4b46-d4ce-032ec865c4bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " model 정의 완료\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# 1) input/target 만들기\n",
        "enc_inputs  = tensor[:, :-1]   # (N, 14)\n",
        "dec_targets = tensor[:,  1:]   # (N, 14)\n",
        "\n",
        "# 2) train(80) + temp(20)\n",
        "enc_train, enc_temp, dec_train, dec_temp = train_test_split(\n",
        "    enc_inputs, dec_targets, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "# 3) temp(20)을 val(10) + test(10)로 나누기\n",
        "enc_val, enc_test, dec_val, dec_test = train_test_split(\n",
        "    enc_temp, dec_temp, test_size=0.5, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(\"train:\", enc_train.shape, dec_train.shape)\n",
        "print(\"val  :\", enc_val.shape, dec_val.shape)\n",
        "print(\"test :\", enc_test.shape, dec_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRctpaS5iGA8",
        "outputId": "1043dd13-1a62-4b52-982b-a584b5762d58"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train: (140768, 14) (140768, 14)\n",
            "val  : (17596, 14) (17596, 14)\n",
            "test : (17596, 14) (17596, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "train_ds = (tf.data.Dataset.from_tensor_slices((enc_train, dec_train))\n",
        "            .cache()\n",
        "            .shuffle(len(enc_train), reshuffle_each_iteration=True)\n",
        "            .batch(BATCH_SIZE, drop_remainder=True)\n",
        "            .prefetch(AUTOTUNE))\n",
        "\n",
        "val_ds = (tf.data.Dataset.from_tensor_slices((enc_val, dec_val))\n",
        "          .cache()\n",
        "          .batch(BATCH_SIZE)\n",
        "          .prefetch(AUTOTUNE))\n",
        "\n",
        "test_ds = (tf.data.Dataset.from_tensor_slices((enc_test, dec_test))\n",
        "           .cache()\n",
        "           .batch(BATCH_SIZE)\n",
        "           .prefetch(AUTOTUNE))\n",
        "\n",
        "print(\" train/val/test dataset ready\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwsUtjWxiGEe",
        "outputId": "83113d56-6f84-4d28-def7-23fd4bbfdb99"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ train/val/test dataset ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FAST_STEPS = 40        # 10~50 추천 (빠른 실험)\n",
        "FAST_VAL_STEPS = 10\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=1, factor=0.5),\n",
        "]\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=5,\n",
        "    steps_per_epoch=FAST_STEPS,\n",
        "    validation_steps=FAST_VAL_STEPS,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqA_D7r9cxGz",
        "outputId": "7ac13e3b-9da3-44b7-aa44-b89c48e5e515"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 241ms/step - loss: 7.6917 - val_loss: 6.2531 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 235ms/step - loss: 6.2155 - val_loss: 6.1404 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 233ms/step - loss: 6.1186 - val_loss: 6.0967 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 235ms/step - loss: 6.0693 - val_loss: 6.0766 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 236ms/step - loss: 6.0674 - val_loss: 6.0612 - learning_rate: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) 테스트평가\n",
        "test_loss = model.evaluate(test_ds)\n",
        "print(\" test_loss:\", test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDwNeAZdcxJq",
        "outputId": "af892a23-d113-4503-ef19-eb0d264dc851"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 77ms/step - loss: 6.0540\n",
            "✅ test_loss: 6.054206371307373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_ilove(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    max_len=30,\n",
        "    temperature=0.85,\n",
        "    top_k=50\n",
        "):\n",
        "    # 강제 시작 규칙\n",
        "    seed = \"<start> i love\"\n",
        "    seq = tokenizer.texts_to_sequences([seed])[0]\n",
        "\n",
        "    START_ID = tokenizer.word_index[\"<start>\"]\n",
        "    END_ID   = tokenizer.word_index[\"<end>\"]\n",
        "\n",
        "    generated = [\"I\", \"love\"]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        x = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [seq[-(MAX_LEN-1):]],\n",
        "            maxlen=(MAX_LEN-1),\n",
        "            padding=\"post\"\n",
        "        )\n",
        "        x = tf.convert_to_tensor(x, dtype=tf.int32)\n",
        "\n",
        "        logits = model(x)[0, -1, :]\n",
        "        logits = logits / temperature\n",
        "\n",
        "        # PAD 제거\n",
        "        logits = tf.tensor_scatter_nd_update(\n",
        "            logits,\n",
        "            indices=[[0]],\n",
        "            updates=[-1e10]\n",
        "        )\n",
        "\n",
        "        # top-k\n",
        "        values, _ = tf.math.top_k(logits, k=top_k)\n",
        "        logits = tf.where(logits < values[-1], -1e10, logits)\n",
        "\n",
        "        next_id = int(tf.random.categorical(\n",
        "            tf.expand_dims(logits, 0), 1\n",
        "        )[0, 0].numpy())\n",
        "\n",
        "        if next_id == END_ID:\n",
        "            break\n",
        "\n",
        "        word = tokenizer.index_word.get(next_id, \"\")\n",
        "        generated.append(word)\n",
        "        seq.append(next_id)\n",
        "\n",
        "    return \" \".join(generated)\n"
      ],
      "metadata": {
        "id": "kDiq5SOWnh0b"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text_ilove(model, tokenizer))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3cXlcdhnm0b",
        "outputId": "24fa7643-aac3-4751-befc-cde8a4dbf836"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I love what me the can\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XzXe7UOOcxRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_QgxV3uybESw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}